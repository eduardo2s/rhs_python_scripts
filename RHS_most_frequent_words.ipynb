{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palavras Mais Frequentes RHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obter os dados das postagens da rede Humaniza SUS\n",
    "2. Tratar o texto obtido\n",
    "3. Fazer a contagem das palavras mais frequentes por dia\n",
    "4. Criar um arquivo com essas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host='localhost',\n",
    "                           port=3306,\n",
    "                           user='root',\n",
    "                           passwd='',\n",
    "                           db='rhs')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT FROM_UNIXTIME(A.created, '%Y') AS Data, C.type AS Tipo, B.comment_body_value AS Comentario FROM comment A INNER JOIN field_data_comment_body B ON A.cid = B.entity_id JOIN node C ON A.nid = C.nid WHERE C.type LIKE 'blog'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazena o nome de cada uma das colunas do banco de dados\n",
    "row_headers=[x[0] for x in cur.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma lista com os dados do banco\n",
    "rhs_data = []\n",
    "for row in cur:\n",
    "    rhs_data.append(dict(zip(row_headers,row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforma a list em um dataframe\n",
    "df = pd.DataFrame(rhs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa o próprio pandas para agrupar os dados por dia/data\n",
    "df_new = pd.DataFrame(df.groupby('Data')['post'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia as datas do groupby do index para uma coluna\n",
    "df_new['Data'] = df_new.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseta o index para ficar númerado de acordo com a ordem dos items\n",
    "df_new = df_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma as Datas em Strings (facilita para trabalhar)\n",
    "df_new['Data'] = df_new['Data'].apply(lambda x: dt.datetime.strftime(x, '%d/%m/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma lista com as postagens que estão dentro do dataframe\n",
    "# cria uma nova lista onde serão tratados caracteres em ASCII e volta a inserir o texto tratado dentro do dataframe\n",
    "dflist = df_new['post'].tolist()\n",
    "\n",
    "# Limpa erros de códificação das postagens (uma vez que tem erros de ASCII não reconhece caracteres como \"ç\")\n",
    "df_html =[]\n",
    "for item in dflist:\n",
    "       parser = HTMLParser()\n",
    "       df_html.append(parser.unescape(item))\n",
    "    \n",
    "# Insere novamente o texto dentro do dataframe\n",
    "df_new['post'] = df_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parte de código mostrada abaixo que contém o BeautifulSoup é muito importante, uma vez que foram identificadas muitas postagens com html, em algumas com muita repetição de tags em html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa o beautifulsoup para limpar todo o html das postagens\n",
    "def cleanMe(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\") # cria um novo objeto do bs4 para o item html\n",
    "    for script in soup([\"script\", \"style\"]): # remove todos scripts e css\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # quebrar em linhas e remove espaço à esquerda e à direita em cada\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # remove linhas em branco\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera uma lista que armazena o texto tratado pela função feita acima\n",
    "df_soup = []\n",
    "for text in df_new.get('post'):\n",
    "       df_soup.append(cleanMe(text))\n",
    "\n",
    "# Insere o texto tratado novamente no dataframe\n",
    "df_new['post'] = df_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento simples de texto, remove caracteres especiais e númericos\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"[\\,\\$\\&\\*\\%\\(\\)\\~\\-\\\"\\^\\+\\#\\/|0-9]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[\\.\\=\\'\\:\\;\\?\\!\\_\\...]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\[[^]]*\\]\", \" \")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "# ativa a função acima que percorre o campo do dataframe com as postagens\n",
    "df_new = standardize_text(df_new, 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover múltiplos espaços em branco tornando o texto em algo único\n",
    "df_aux = []\n",
    "for word in df_new.get('post'):\n",
    "       df_aux.append(word.split())\n",
    "\n",
    "# Juntar novamente as palavras de cada um dos post para o texto ficar uniforme\n",
    "space_correction = [ ' '.join(l) for l in df_aux]\n",
    "\n",
    "df_new['post'] = final_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# Por serem textos mais completos usar um arquivo próprio para stopwords\n",
    "pathStopwords = \"stop_words.txt\"\n",
    "\n",
    "# Read in and split the stopwords file.\n",
    "with open(pathStopwords, 'r') as f:\n",
    "    stop_words = f.read().split(\"\\n\")\n",
    "\n",
    "# transforma as postagens que estão no dataframe em lista para retirar as stopwords\n",
    "dflist = df_new['post'].tolist()\n",
    "\n",
    "my_new_list = [[word for word in text.split() if word not in stop_words] for text in dflist]\n",
    "my_new_list = [ ' '.join(l) for l in my_new_list]\n",
    "df_new['post'] = my_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista com as palavras separadas entre si\n",
    "wordlist = []\n",
    "for wordlists in df_new.get('post'):\n",
    "    wordlist.append(wordlists.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista com a frequência das palavras, armazenando elas de acordo com o dia da postagem\n",
    "wordfreq = []\n",
    "c = 0\n",
    "for c in range(len(wordlist)):\n",
    "    if c <= 3228:\n",
    "        wordfreq.append(Counter(wordlist[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista com as 10 palavras mais frequentes\n",
    "wordfreqmost = []\n",
    "for counter in wordfreq:\n",
    "    wordfreqmost.append(Counter(counter).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa o output que recebemos é uma lista de tuplas, quando transformamos essa lista em dataframe ela gera uma coluna para cada palavra e o respectivo número de aparições, como verificamos as 10 mais frequentes, são geradas 10 colunas no dataframe, onde renomeamos as colunas com letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe com as palavras que foram geradas na lista anterior\n",
    "df_words = pd.DataFrame(wordfreqmost)\n",
    "df_words.columns = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todas as colunas para que as palavras fiquem juntas\n",
    "d = list(zip(df_words.A,df_words.B,df_words.C,df_words.D,df_words.E,df_words.F,\n",
    "             df_words.G,df_words.H,df_words.I,df_words.J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe final vazio, insere a Data da postagem nesse novo dataframe\n",
    "# e a coluna que contem todas as palavras com o número de aparições juntas\n",
    "df_final = pd.DataFrame()\n",
    "df_final['Data'] = df_new['Data']\n",
    "df_final['Concatenado'] = pd.Series(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos o arquivo CSV final que será tratado para que os dados sejam colocados no Tableau\n",
    "df_final.to_csv(\"rhs_word_cloud.csv\", encoding='UTF-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
